{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# En local!! Ollama + llama2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#conda create -n langchain python=3.12 -y\n",
    "#conda activate langchain\n",
    "#pip3 install torch torchvision\n",
    "#pip3 install langchain langchain-community pypdf openai chromadb tiktoken sentence-transformers transformers ipykernel\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#uv venv\n",
    "#\".venv/Scipts/activate.bat\"\n",
    "# uv pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cpu\n",
    "#uv pip install langchain langchain-community pypdf openai chromadb tiktoken sentence-transformers transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\lftob\\miniconda3\\envs\\langchain\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Descargado paper1.pdf\n",
      "Descargado paper2.pdf\n",
      "Descargado paper3.pdf\n",
      "Descargado paper4.pdf\n",
      "Descargado paper5.pdf\n",
      "Contenido de ml_papers:\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "\n",
    "urls = [\n",
    "    'https://arxiv.org/pdf/2306.06031v1.pdf',\n",
    "    'https://arxiv.org/pdf/2306.12156v1.pdf',\n",
    "    'https://arxiv.org/pdf/2306.14289v1.pdf',\n",
    "    'https://arxiv.org/pdf/2305.10973v1.pdf',\n",
    "    'https://arxiv.org/pdf/2306.13643v1.pdf'\n",
    "]\n",
    "\n",
    "ml_papers = []\n",
    "\n",
    "for i, url in enumerate(urls):\n",
    "    response = requests.get(url)\n",
    "    filename = f'paper{i+1}.pdf'\n",
    "    with open(filename, 'wb') as f:\n",
    "        f.write(response.content)\n",
    "        print(f'Descargado {filename}')\n",
    "\n",
    "        loader = PyPDFLoader(filename)\n",
    "        data = loader.load()\n",
    "        ml_papers.extend(data)\n",
    "\n",
    "# Utiliza la lista ml_papers para acceder a los elementos de todos los documentos descargados\n",
    "print('Contenido de ml_papers:')\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(list,\n",
       " 57,\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2023-06-12T00:32:18+00:00', 'author': '', 'keywords': '', 'moddate': '2023-06-12T00:32:18+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'templateversion': 'IJCAI.2023.0', 'title': '', 'trapped': '/False', 'source': 'paper1.pdf', 'total_pages': 7, 'page': 3, 'page_label': '4'}, page_content='Figure 1: FinGPT Framework.\\n4.1 Data Sources\\nThe first stage of the FinGPT pipeline involves the collec-\\ntion of extensive financial data from a wide array of online\\nsources. These include, but are not limited to:\\n• Financial news: Websites such as Reuters, CNBC, Yahoo\\nFinance, among others, are rich sources of financial news\\nand market updates. These sites provide valuable informa-\\ntion on market trends, company earnings, macroeconomic\\nindicators, and other financial events.\\n• Social media: Platforms such as Twitter, Facebook, Red-\\ndit, Weibo, and others, offer a wealth of information in\\nterms of public sentiment, trending topics, and immediate\\nreactions to financial news and events.\\n• Filings: Websites of financial regulatory authorities, such\\nas the SEC in the United States, offer access to company\\nfilings. These filings include annual reports, quarterly earn-\\nings, insider trading reports, and other important company-\\nspecific information. Official websites of stock exchanges\\n(NYSE, NASDAQ, Shanghai Stock Exchange, etc.) pro-\\nvide crucial data on stock prices, trading volumes, company\\nlistings, historical data, and other related information.\\n• Trends: Websites like Seeking Alpha, Google Trends, and\\nother finance-focused blogs and forums provide access to\\nanalysts’ opinions, market predictions, the movement of\\nspecific securities or market segments and investment ad-\\nvice.\\n• Academic datasets: Research-based datasets that offer cu-\\nrated and verified information for sophisticated financial\\nanalysis.\\nTo harness the wealth of information from these diverse\\nsources, FinGPT incorporates data acquisition tools capable\\nof scraping structured and unstructured data, including APIs,\\nweb scraping tools, and direct database access where avail-\\nable. Moreover, the system is designed to respect the terms\\nof service of these platforms, ensuring data collection is ethi-\\ncal and legal.\\nData APIs: In the FinGPT framework, APIs are used not\\nonly for initial data collection but also for real-time data up-\\ndates, ensuring the model is trained on the most current data.\\nAdditionally, error handling and rate-limiting strategies are\\nimplemented to respect API usage limits and avoid disrup-\\ntions in the data flow.\\n4.2 Real-Time Data Engineering Pipeline for\\nFinancial NLP\\nFinancial markets operate in real-time and are highly sensi-\\ntive to news and sentiment. Prices of securities can change\\nrapidly in response to new information, and delays in pro-\\ncessing that information can result in missed opportunities or\\nincreased risk. As a result, real-time processing is essential in\\nfinancial NLP.\\nThe primary challenge with a real-time NLP pipeline is\\nmanaging and processing the continuous inflow of data ef-\\nficiently. The first step in the pipeline is to set up a system to'))"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(ml_papers), len(ml_papers), ml_papers[3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Split de documentos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=1500,  #Tamaño del texto de cada chunk\n",
    "    chunk_overlap=200, #Hace que al principio de cada chunk esten 200 caracteres del anterior, para dar continuidad\n",
    "    length_function=len #Se hace que el chunk sea por longitud\n",
    "    # Hay una forma de evaluar los chunks https://chunkviz.up.railway.app/\n",
    "    )\n",
    "\n",
    "documents = text_splitter.split_documents(ml_papers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(211,\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2023-06-12T00:32:18+00:00', 'author': '', 'keywords': '', 'moddate': '2023-06-12T00:32:18+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'templateversion': 'IJCAI.2023.0', 'title': '', 'trapped': '/False', 'source': 'paper1.pdf', 'total_pages': 7, 'page': 2, 'page_label': '3'}, page_content='highly volatile, changing rapidly in response to news events\\nor market movements.\\nTrends, often observable through websites like Seeking\\nAlpha, Google Trends, and other finance-oriented blogs and\\nforums, offer critical insights into market movements and in-\\nvestment strategies. They feature:\\n• Analyst perspectives: These platforms provide access to\\nmarket predictions and investment advice from seasoned\\nfinancial analysts and experts.\\n• Market sentiment: The discourse on these platforms can\\nreflect the collective sentiment about specific securities,\\nsectors, or the overall market, providing valuable insights\\ninto the prevailing market mood.\\n• Broad coverage: Trends data spans diverse securities and\\nmarket segments, offering comprehensive market coverage.\\nEach of these data sources provides unique insights into\\nthe financial world. By integrating these diverse data types,\\nfinancial language models like FinGPT can facilitate a com-\\nprehensive understanding of financial markets and enable ef-\\nfective financial decision-making.\\n3.2 Challenges in Handling Financial Data\\nWe summarize three major challenges for handling financial\\ndata as follows:\\n• High temporal sensitivity : Financial data are character-\\nized by their time-sensitive nature. Market-moving news or\\nupdates, once released, provide a narrow window of oppor-\\ntunity for investors to maximize their alpha (the measure of\\nan investment’s relative return).\\n• High dynamism : The financial landscape is perpetually'))"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(documents), documents[10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Embeddings e ingesta de datos vectorial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "hf_token = os.getenv(\"HUGGING_FACE_TOKEN\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\lftob\\AppData\\Local\\Temp\\ipykernel_17836\\1419044909.py:6: LangChainDeprecationWarning: The class `HuggingFaceEmbeddings` was deprecated in LangChain 0.2.2 and will be removed in 1.0. An updated version of the class exists in the `langchain-huggingface package and should be used instead. To use it run `pip install -U `langchain-huggingface` and import as `from `langchain_huggingface import HuggingFaceEmbeddings``.\n",
      "  embeddings = HuggingFaceEmbeddings(\n",
      "c:\\Users\\lftob\\miniconda3\\envs\\langchain\\Lib\\site-packages\\huggingface_hub\\file_download.py:130: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\lftob\\.cache\\huggingface\\hub\\models--BAAI--bge-large-en-v1.5. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n",
      "Loading weights: 100%|██████████| 391/391 [00:00<00:00, 861.05it/s, Materializing param=pooler.dense.weight]                               \n",
      "\u001b[1mBertModel LOAD REPORT\u001b[0m from: BAAI/bge-large-en-v1.5\n",
      "Key                     | Status     |  | \n",
      "------------------------+------------+--+-\n",
      "embeddings.position_ids | UNEXPECTED |  | \n",
      "\n",
      "\u001b[3mNotes:\n",
      "- UNEXPECTED\u001b[3m\t:can be ignored when loading from different task/architecture; not ok if you expect identical arch.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "from langchain_community.embeddings import HuggingFaceEmbeddings\n",
    "from langchain_community.vectorstores import Chroma\n",
    "\n",
    "chunks = text_splitter.split_documents(ml_papers) #documents\n",
    "\n",
    "embeddings = HuggingFaceEmbeddings(\n",
    "    model_name=\"BAAI/bge-large-en-v1.5\"\n",
    ")\n",
    "\n",
    "vectorstore = Chroma.from_documents(\n",
    "    documents=chunks,\n",
    "    embedding=embeddings,\n",
    "    persist_directory=\"./chroma_db\"\n",
    ")\n",
    "# Un retriever convierte la base de datos \"retorna\" los fragmentos clave para responder la pregunta\n",
    "# retriever = vectorstore.as_retriever(\n",
    "#     search_kwargs={\"k\": 3}\n",
    "# )\n",
    "\n",
    "# 2. Mejora el Retriever\n",
    "# Aumentamos k a 5 para dar más contexto y usamos \"mmr\" (Maximum Marginal Relevance)\n",
    "# para evitar fragmentos que digan lo mismo y obtener información diversa.\n",
    "retriever = vectorstore.as_retriever(\n",
    "    search_type=\"mmr\",\n",
    "    search_kwargs={\"k\": 5, \"fetch_k\": 10}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.chat_models import ChatOllama\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "llm = ChatOllama(\n",
    "    model=\"qwen2.5:7b\", #qwen3-vl:4b\n",
    "    temperature=0\n",
    ")\n",
    "\n",
    "prompt = ChatPromptTemplate.from_template(\"\"\"\n",
    "Responde la pregunta usando exclusivamente el contexto.\n",
    "Si no está en el contexto, di literalmente y únicamente \"No lo sé\" y nada más.\n",
    "                                          \n",
    "\n",
    "Contexto:\n",
    "{context}\n",
    "\n",
    "Pregunta:\n",
    "{question}\n",
    "\"\"\")\n",
    "\n",
    "def format_docs(docs):\n",
    "    return \"\\n\\n\".join(doc.page_content for doc in docs)\n",
    "\n",
    "qa_chain = (\n",
    "    {\n",
    "        \"context\": retriever | format_docs,\n",
    "        \"question\": lambda x: x\n",
    "    }\n",
    "    | prompt\n",
    "    | llm\n",
    "    | StrOutputParser()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'FinGPT es un marco de trabajo abierto y gratuito para modelos de lenguaje grande especializados en finanzas (FinLLMs). Este marco adopta una abordaje centrado en los datos, enfatizando la importancia de la adquisición, limpieza y preprocesamiento de datos. FinGPT tiene como objetivo democratizar el acceso a los datos financieros y los modelos FinLLMs, promoviendo la investigación, colaboración e innovación en finanzas. Proporciona una serie de aplicaciones prácticas para tareas financieras, incluyendo asesoramiento robótico, trading cuantitativo, desarrollo de bajo código, detección de fraude financiero, calificación crediticia, predicción de insolvencia, pronóstico de M&A y evaluación ESG. Además, ofrece una interfaz amigable para el desarrollo de software y sirve como un tutor en educación financiera.'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "qa_chain.invoke(\"¿Qué es FinGPT?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Es difícil entrenar FinGPT debido a los desafíos en la obtención de datos de calidad, el manejo de diversos formatos y tipos de datos, y la gestión de inconsistencias en la calidad de los datos. Especialmente, la extracción de datos históricos o especializados puede ser compleja debido a los medios de datos variados como plataformas web, APIs, documentos PDF y imágenes.'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "qa_chain.invoke(\"Porqué es dificil entrenar FinGPT?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'FastSAM es un método para la tarea de segmentación todo-en-un (segment anything), que se divide en dos etapas principales: la segmentación de todas las instancias y la selección guiada por el prompt. La primera etapa detecta y segmenta todos los objetos en la imagen, similar al proceso de segmentación panóptica. La segunda etapa utiliza los prompts proporcionados para separar los objetos específicos de interés del panorama segmentado. Este enfoque decupla la tarea, reduciendo su complejidad y permitiendo la propuesta de un modelo de segmentación todo-en-un en tiempo real.'"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "qa_chain.invoke(\"qué es fast segment anything?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'FastSAM tiene 68M parámetros y tarda 40ms en procesar una imagen, mientras que MobileSAM tiene menos de 10M parámetros, lo cual es significativamente más pequeño, y solo toma 10ms para procesar una imagen, lo que es 4 veces más rápido que FastSAM. Además, el mIoU de MobileSAM es mucho mayor que el de FastSAM, lo que indica un mejor rendimiento en la predicción de máscaras.'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "qa_chain.invoke(\"cuál es la diferencia entre fast sam y mobile sam?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'No lo sé'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "qa_chain.invoke(\"cuál es la temperatura aproximada de la superficie del sol?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "langchain",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
